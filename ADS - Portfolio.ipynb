{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: right\">Generative Adversarial Networks to create new Simpsons character face</div>\n",
    "\n",
    "<div style=\"text-align: right\">by Neel Indap(indap.n@husky.neu.edu)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Generative Adversarial Network (GAN)?\n",
    "Generative Adversarial Network (referred to as GAN) is a network that generates new data with the same internal structure as the training data. They can be described as generative models based on supervised learning.\n",
    "It consists of 2 Neural Networks models, the generator (which defines takes random noise and generates samples), and a discriminator (which takes the above sample, and tries to determine if it is fake or real). At each step, we try to minimize the loss for both models, until the point where the generator produces samples virtually indistinguishable from the real images, for the discriminator.\n",
    "The network itself can be thought of as a game between the 2 models, both competing to win.\n",
    "To gain more insight into this, refer to the following paper published by Ian Goodfellow and his colleagues, explaining their motivation behind this.<br>\n",
    "[GAN paper](https://arxiv.org/abs/1406.2661)\n",
    "\n",
    "If you are new to Neural Networks, checkout this video. It is a good starting point in understanding what they are and how do they work.<br>\n",
    "[What is a Neural Network?](https://www.youtube.com/watch?v=aircAruvnKk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Architecture\n",
    "***\n",
    "![GAN Architecture](./images/GAN_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why GAN?\n",
    "GANs are cited as the most interesting idea in the last ten years by the Yann LeCun, the director of AI at Facebook. This intrigued me to understand the working of this algorithm.\n",
    "<br>Since its inception, there have been various improvements published. Most of these are around image generation.\n",
    "In this paper, I am trying to train the model using a custom image set of only 100 images as training data. The original paper used a CelebA dataset provided by imagenet consisting of 200k images.\n",
    "\n",
    "Trying to get a stable working model using a small dataset, I am trying to see the impact of changing the hyper parameters, as well as modifying the neural network itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements on GAN - DCGAN\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortly after its inception there was a paper published called [Unsupervised Learning using Deep Convolution GAN](https://arxiv.org/abs/1511.06434).\n",
    "\n",
    "This paper talks about the use of batch normalization in the CNN layers to improve preformance of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up code\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is hosted on [Github](https://github.com/neelindap/DCGAN-tensorflow)\n",
    "\n",
    "Clone the repository using ``` git clone https://github.com/neelindap/DCGAN-tensorflow```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cloning the repository, please install the following dependency:\n",
    "``` pip install Pillow ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_NOTE_**:<br>\n",
    "\n",
    "It is assumed the system already has Tensorflow env set up. It not, refer to the this [tutorial](https://www.tensorflow.org/install/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Code\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the code, on your terminal navigate to the installed path and run\n",
    "``` python main.py --train --crop ```\n",
    "\n",
    "This will automatically pick-up the training images present in the folder ```./Data/Simpsons_64```.\n",
    "In order to use a different data set, place the images in the folder ```./Data``` folder and change the name of the \"dataset\" flag in ```main.py``` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN model - Tensorboard Visualization\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tensorflow Visualization](./images/GAN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Snippets\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Generator](./images/Generator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Discriminator](./images/Discriminator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator and Discriminator models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Generator and Discriminator models are formed as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------<br>Variables: name (type shape) [size]<br>---------<br>generator/g_h0_lin/Matrix:0 (float32_ref 100x16384) [1638400, bytes: 6553600]<br>generator/g_h0_lin/bias:0 (float32_ref 16384) [16384, bytes: 65536]<br>generator/g_bn0/beta:0 (float32_ref 1024) [1024, bytes: 4096]<br>generator/g_bn0/gamma:0 (float32_ref 1024) [1024, bytes: 4096]<br>generator/g_h1/w:0 (float32_ref 5x5x512x1024) [13107200, bytes: 52428800]<br>generator/g_h1/biases:0 (float32_ref 512) [512, bytes: 2048]<br>generator/g_bn1/beta:0 (float32_ref 512) [512, bytes: 2048]<br>generator/g_bn1/gamma:0 (float32_ref 512) [512, bytes: 2048]<br>generator/g_h2/w:0 (float32_ref 5x5x256x512) [3276800, bytes: 13107200]<br>generator/g_h2/biases:0 (float32_ref 256) [256, bytes: 1024]<br>generator/g_bn2/beta:0 (float32_ref 256) [256, bytes: 1024]<br>generator/g_bn2/gamma:0 (float32_ref 256) [256, bytes: 1024]<br>generator/g_h3/w:0 (float32_ref 5x5x128x256) [819200, bytes: 3276800]<br>generator/g_h3/biases:0 (float32_ref 128) [128, bytes: 512]<br>generator/g_bn3/beta:0 (float32_ref 128) [128, bytes: 512]<br>generator/g_bn3/gamma:0 (float32_ref 128) [128, bytes: 512]<br>generator/g_h4/w:0 (float32_ref 5x5x64x128) [204800, bytes: 819200]<br>generator/g_h4/biases:0 (float32_ref 64) [64, bytes: 256]<br>generator/g_bn4/beta:0 (float32_ref 64) [64, bytes: 256]<br>generator/g_bn4/gamma:0 (float32_ref 64) [64, bytes: 256]<br>generator/g_h5/w:0 (float32_ref 5x5x3x64) [4800, bytes: 19200]<br>generator/g_h5/biases:0 (float32_ref 3) [3, bytes: 12]<br>discriminator/d_h0_conv/w:0 (float32_ref 5x5x3x64) [4800, bytes: 19200]<br>discriminator/d_h0_conv/biases:0 (float32_ref 64) [64, bytes: 256]<br>discriminator/d_h1_conv/w:0 (float32_ref 5x5x64x128) [204800, bytes: 819200]<br>discriminator/d_h1_conv/biases:0 (float32_ref 128) [128, bytes: 512]<br>discriminator/d_bn1/beta:0 (float32_ref 128) [128, bytes: 512]<br>discriminator/d_bn1/gamma:0 (float32_ref 128) [128, bytes: 512]<br>discriminator/d_h2_conv/w:0 (float32_ref 5x5x128x256) [819200, bytes: 3276800]<br>discriminator/d_h2_conv/biases:0 (float32_ref 256) [256, bytes: 1024]<br>discriminator/d_bn2/beta:0 (float32_ref 256) [256, bytes: 1024]<br>discriminator/d_bn2/gamma:0 (float32_ref 256) [256, bytes: 1024]<br>discriminator/d_h3_conv/w:0 (float32_ref 5x5x256x512) [3276800, bytes: 13107200]<br>discriminator/d_h3_conv/biases:0 (float32_ref 512) [512, bytes: 2048]<br>discriminator/d_bn3/beta:0 (float32_ref 512) [512, bytes: 2048]<br>discriminator/d_bn3/gamma:0 (float32_ref 512) [512, bytes: 2048]<br>discriminator/d_h4_conv/w:0 (float32_ref 5x5x512x1024) [13107200, bytes: 52428800]<br>discriminator/d_h4_conv/biases:0 (float32_ref 1024) [1024, bytes: 4096]<br>discriminator/d_bn4/beta:0 (float32_ref 1024) [1024, bytes: 4096]<br>discriminator/d_bn4/gamma:0 (float32_ref 1024) [1024, bytes: 4096]<br>discriminator/d_h5_lin/Matrix:0 (float32_ref 16384x1) [16384, bytes: 65536]<br>discriminator/d_h5_lin/bias:0 (float32_ref 1) [1, bytes: 4]<br>Total size of variables: 36507524<br>Total bytes of variables: 146030096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the loss functions for the 2 models as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ```python\n",
    "d_loss_real = tf.reduce_mean(\n",
    "      sigmoid_cross_entropy_with_logits(D_logits, tf.ones_like(D)))\n",
    "      \n",
    "d_loss_fake = tf.reduce_mean(\n",
    "      sigmoid_cross_entropy_with_logits(D_logits_, tf.zeros_like(D_)))\n",
    "\n",
    "g_loss = tf.reduce_mean(\n",
    "      sigmoid_cross_entropy_with_logits(D_logits_, tf.ones_like(D_)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where, <br>\n",
    "d_loss_real is the loss for the real images passing through the discriminator<br>\n",
    "d_loss_fake is the loss for the fake images passing through the discriminator<br>\n",
    "g_loss is the loss for the images generated by the generator<br>\n",
    "\n",
    "The total loss of the discriminator (d_loss) is the sum of d_loss_real and d_loss_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Adam Optimizer to optimize the generator and discriminator models. They are defined as follows:<br>\n",
    "```python\n",
    "d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1).minimize(self.d_loss, var_list=self.d_vars)\n",
    "g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1).minimize(self.g_loss, var_list=self.g_vars)\n",
    "```\n",
    "<br>\n",
    "Learning Rate is 0.0001 and decay(beta1) is 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tActivation Function: tanH for Generator and Sigmoid for Discriminator\n",
    "2.\tCost Function: Sigmoid with Cross Entorpy\n",
    "3.\tGradient Descent: Adam Optimizer with learning rate: 0.0001 & beta1(decay rate of 1st moment estimation): 0.5\n",
    "4.\tNetwork Architecture: 5-layer Neural Network\n",
    "5.\tNetwork Initializer: random normal initializer\n",
    "6.\tBatch Size: 25\n",
    "7.\tTotal Images: 100\n",
    "8.\tEpochs: 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model in training\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gist of the model training code is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "    # Update D network\n",
    "     _, summary_str = self.sess.run([d_optim, self.d_sum],\n",
    "    feed_dict={ self.inputs: batch_images, self.z: batch_z })\n",
    "    # Update G network\n",
    "    _, summary_str = self.sess.run([g_optim, self.g_sum],\n",
    "    feed_dict={ self.z: batch_z })\n",
    "\n",
    "    # Run g_optim twice to make sure that d_loss does not go to zero (different from paper)\n",
    "    _, summary_str = self.sess.run([g_optim, self.g_sum],\n",
    "    feed_dict={ self.z: batch_z })\n",
    "          \n",
    "    errD_fake = self.d_loss_fake.eval({ self.z: batch_z })\n",
    "    errD_real = self.d_loss_real.eval({ self.inputs: batch_images })\n",
    "    errG = self.g_loss.eval({self.z: batch_z})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At every step, we try to optimize the sum of the network, where the sum is the sum of the losses. In the discriminator's case, it is the loss of the real images, and in generator's case, it is the loss of the fake images.<br>\n",
    "It is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "self.g_sum = tf.summary.merge([self.z_sum, self.d__sum,\n",
    "      self.G_sum, self.d_loss_fake_sum, self.g_loss_sum])\n",
    "self.d_sum = tf.summary.merge([self.z_sum, self.d_sum, \n",
    "      self.d_loss_real_sum, self.d_loss_sum])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The losses are captured in Tensorboard:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Discriminator loss](./images/d_loss.PNG)\n",
    "**Fig. 1 : Discriminator Loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Generator loss](./images/g_loss.PNG)\n",
    "**Fig. 2 : Generator Loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the loss of the 2 are kind of inversely related (Like adversaries). <br>\n",
    "If the discriminator has a lower loss, it means it can distinguish the fake images from the real ones, which in turn means the generator cannot produce good quality output, and vice-versa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Output\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GAN](./images/GAN.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Output\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Test Output](./images/test_20180425010142.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the output generated by the test, it is evident that the model had started to distinguish between various Simpson’s characters and tried to generate a new face based off the existing ones.\n",
    "The model did lose its track around 600 epoch, where in started generating noise instead of faces. It stabilized in some 400 epochs, and eventually started producing better outputs again around the 1000 epoch.\n",
    "\n",
    "With enough images and more training, I think the model would be stable enough to generate better output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Scope\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GAN model while have many applications still isn’t stable enough to generate definitive results.<br>\n",
    "Model is susceptible to mode collapse, where in once the generator can fool the discriminator, it keeps on producing similar results again and again. <br><br>\n",
    "GAN models also suffer from convergence, and therefore we don’t know when to stop training. To overcome this, there was a paper proposing use of Wasserstein distance instead of Jensen-Shannon divergence to understand the loss function better, which can be correlated to image quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Test Output](./images/WGAN.png)\n",
    "**Fig. 3: Loss functions in WGAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another new search in the field of neural networks gave rise to Capsule Networks, which are evidently much better than CNNs in training models.<br>\n",
    "These networks can be used in place of CNNs in the GAN architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tGenerative Adversarial Networks (https://arxiv.org/abs/1406.2661)<br>\n",
    "2.\tGAN tutorial: https://medium.com/@awjuliani/generative-adversarial-networks-explained-with-a-classic-spongebob-squarepants-episode-54deab2fce39.<br>\n",
    "3.\tGenerative models: https://en.wikipedia.org/wiki/Generative_model<br>\n",
    "4.\tDiscriminative models : https://en.wikipedia.org/wiki/Discriminative_model<br>\n",
    "5.\tCNN: http://cs231n.github.io/convolutional-networks/<br>\n",
    "6.\tDCGAN: https://github.com/carpedm20/DCGAN-tensorflow<br>\n",
    "7.\tHacks for GAN: https://github.com/soumith/ganhacks<br>\n",
    "8.\thttps://arxiv.org/abs/1511.06434<br>\n",
    "9.\thttps://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6<br>\n",
    "10. https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/<br>\n",
    "11. http://gluon.mxnet.io/chapter14_generative-adversarial-networks/dcgan.html<br>\n",
    "12. WGAN https://arxiv.org/abs/1701.07875<br>\n",
    "13. https://hackernoon.com/what-is-a-capsnet-or-capsule-network-2bfbe48769cc<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Licenses\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text in the document by Neel Indap is licensed under CC BY 3.0 https://creativecommons.org/licenses/by/3.0/us/\n",
    "\n",
    "The code in the document by Neel Indap is licensed under the MIT License https://opensource.org/licenses/MIT\n",
    "\n",
    "![License](https://licensebuttons.net/l/by/3.0/us/88x31.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
