{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: right\">Generative Adversarial Networks to create new Simpsons character face</div>\n",
    "\n",
    "<div style=\"text-align: right\">by Neel Indap(indap.n@husky.neu.edu)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Generative Adversarial Network (GAN)?\n",
    "Generative Adversarial Network (referred to as GAN) is a network that generates new data with the same internal structure as the training data. They can be described as generative models based on supervised learning.\n",
    "It consists of 2 Neural Networks models, the generator (which defines takes random noise and generates samples), and a discriminator (which takes the above sample, and tries to determine if it is fake or real). At each step, we try to minimize the loss for both models, until the point where the generator produces samples virtually indistinguishable from the real images, for the discriminator.\n",
    "The network itself can be thought of as a game between the 2 models, both competing to win.\n",
    "To gain more insight into this, refer to the following paper published by Ian Goodfellow and his colleagues, explaining their motivation behind this.<br>\n",
    "[GAN paper](https://arxiv.org/abs/1406.2661)\n",
    "\n",
    "If you are new to Neural Networks, checkout this video. It is a good starting point in understanding what they are and how do they work.<br>\n",
    "[What is a Neural Network?](https://www.youtube.com/watch?v=aircAruvnKk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Architecture\n",
    "***\n",
    "![GAN Architecture](./images/GAN_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why GAN?\n",
    "GANs are cited as the most interesting idea in the last ten years by the Yann LeCun, the director of AI at Facebook. This intrigued me to understand the working of this algorithm.<br>\n",
    "Since its inception, there have been various improvements published. Most of these are around image generation.\n",
    "\n",
    "In this paper, I am trying to train the model using a custom image set of only 100 images as training data. The original paper used a CelebA dataset provided by imagenet consisting of 200k images.<br>\n",
    "Trying to get a stable working model using a small dataset, and noting the impact of changing the hyper parameters, as well as modifying the neural network itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements on GAN - DCGAN\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortly after the initial concept was proposed, there was a paper published called [Unsupervised Learning using Deep Convolution GAN](https://arxiv.org/abs/1511.06434).\n",
    "\n",
    "This paper talks about the use of batch normalization in the CNN layers to improve preformance of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code setup\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is hosted on [Github](https://github.com/neelindap/DCGAN-tensorflow)\n",
    "\n",
    "Clone the repository using ``` git clone https://github.com/neelindap/DCGAN-tensorflow```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cloning the repository, please install the following dependency:\n",
    "``` pip install Pillow ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_NOTE_**:<br>\n",
    "\n",
    "It is assumed the system already has Tensorflow env set up. It not, refer to the this [tutorial](https://www.tensorflow.org/install/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used the following set of images I found as the training data: [Simpsons dataset](https://github.com/jbencina/simpsons-image-training-dataset)\n",
    "\n",
    "The original paper refers to the CelebA dataset, which contains approximately 200k images, cropped and aligned.\n",
    "The dataset for the Simpsons, contains about 990 images in one and 2500 images in the other, totaling approximately 3400 images.\n",
    "The image dataset is only a fraction of that used for the original model. Also, the images themselves aren’t purely faces. Majority of the images contains body along with the face, more than one face or even noisy background which makes isolating the faces from the images difficult.\n",
    "\n",
    "I tried using ***Harr Cascade*** to isolate the faces from the images. Harr Cascade has previously shown good results in detecting human faces.\n",
    "The challenging part here was since Simpsons character face didn’t have precisely similar characteristics as human faces, the filters written for Harr Cascade to detect human faces didn’t work well on Simpsons dataset.\n",
    "On running the classifier on the dataset, only 6 images out of the 3400 images were detected and cropped.\n",
    "Even with different Harr Cascade filters (frontalface_alt, frontalface_alt2) the results weren’t any good.\n",
    "\n",
    "Owing to hardware limitations, I picked up 100 images which were of the best quality and good orientation and began training the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Pre-processing\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After selecting the images, I wrote a Python script to resize the images to 64x64.<br>\n",
    "As these are colored images, the resulting vector was of the size 64x64x3.<br><br>\n",
    "The original size of the images combined with 3 channels would have resulted in a vector of very large size and would’ve resulted in longer computation times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for resizing images\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for file in os.listdir(dir):\n",
    "    filename = os.fsdecode(os.path.join(dir,file))\n",
    "\n",
    "    img = Image.open(filename)\n",
    "    img = img.resize((basewidth, hsize), PIL.Image.ANTIALIAS)\n",
    "    img.save(os.path.join(new_path,str(i)+\".jpg\"))\n",
    "    i += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code scans through the source folder, opening one image at a time and reszing them and saving the new images at the destination path.<br>\n",
    "I used ```Pillow library``` to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN model - Tensorboard Visualization\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tensorflow Visualization](./images/GAN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE :**<br>\n",
    "\n",
    "The major functionality of the code is written in the file ```model.py``` in the source code you've downloaded. <br> \n",
    "You can find it on [Github](https://github.com/neelindap/DCGAN-tensorflow/blob/master/model.py).\n",
    "\n",
    "I've explained the important snippets from the model below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Model\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Generator](./images/Generator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Model\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Discriminator](./images/Discriminator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator and Discriminator models\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Generator and Discriminator models are formed as follows when the code runs :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------<br>Variables: name (type shape) [size]<br>---------<br>generator/g_h0_lin/Matrix:0 (float32_ref 100x16384) [1638400, bytes: 6553600]<br>generator/g_h0_lin/bias:0 (float32_ref 16384) [16384, bytes: 65536]<br>generator/g_bn0/beta:0 (float32_ref 1024) [1024, bytes: 4096]<br>generator/g_bn0/gamma:0 (float32_ref 1024) [1024, bytes: 4096]<br>generator/g_h1/w:0 (float32_ref 5x5x512x1024) [13107200, bytes: 52428800]<br>generator/g_h1/biases:0 (float32_ref 512) [512, bytes: 2048]<br>generator/g_bn1/beta:0 (float32_ref 512) [512, bytes: 2048]<br>generator/g_bn1/gamma:0 (float32_ref 512) [512, bytes: 2048]<br>generator/g_h2/w:0 (float32_ref 5x5x256x512) [3276800, bytes: 13107200]<br>generator/g_h2/biases:0 (float32_ref 256) [256, bytes: 1024]<br>generator/g_bn2/beta:0 (float32_ref 256) [256, bytes: 1024]<br>generator/g_bn2/gamma:0 (float32_ref 256) [256, bytes: 1024]<br>generator/g_h3/w:0 (float32_ref 5x5x128x256) [819200, bytes: 3276800]<br>generator/g_h3/biases:0 (float32_ref 128) [128, bytes: 512]<br>generator/g_bn3/beta:0 (float32_ref 128) [128, bytes: 512]<br>generator/g_bn3/gamma:0 (float32_ref 128) [128, bytes: 512]<br>generator/g_h4/w:0 (float32_ref 5x5x64x128) [204800, bytes: 819200]<br>generator/g_h4/biases:0 (float32_ref 64) [64, bytes: 256]<br>generator/g_bn4/beta:0 (float32_ref 64) [64, bytes: 256]<br>generator/g_bn4/gamma:0 (float32_ref 64) [64, bytes: 256]<br>generator/g_h5/w:0 (float32_ref 5x5x3x64) [4800, bytes: 19200]<br>generator/g_h5/biases:0 (float32_ref 3) [3, bytes: 12]<br>discriminator/d_h0_conv/w:0 (float32_ref 5x5x3x64) [4800, bytes: 19200]<br>discriminator/d_h0_conv/biases:0 (float32_ref 64) [64, bytes: 256]<br>discriminator/d_h1_conv/w:0 (float32_ref 5x5x64x128) [204800, bytes: 819200]<br>discriminator/d_h1_conv/biases:0 (float32_ref 128) [128, bytes: 512]<br>discriminator/d_bn1/beta:0 (float32_ref 128) [128, bytes: 512]<br>discriminator/d_bn1/gamma:0 (float32_ref 128) [128, bytes: 512]<br>discriminator/d_h2_conv/w:0 (float32_ref 5x5x128x256) [819200, bytes: 3276800]<br>discriminator/d_h2_conv/biases:0 (float32_ref 256) [256, bytes: 1024]<br>discriminator/d_bn2/beta:0 (float32_ref 256) [256, bytes: 1024]<br>discriminator/d_bn2/gamma:0 (float32_ref 256) [256, bytes: 1024]<br>discriminator/d_h3_conv/w:0 (float32_ref 5x5x256x512) [3276800, bytes: 13107200]<br>discriminator/d_h3_conv/biases:0 (float32_ref 512) [512, bytes: 2048]<br>discriminator/d_bn3/beta:0 (float32_ref 512) [512, bytes: 2048]<br>discriminator/d_bn3/gamma:0 (float32_ref 512) [512, bytes: 2048]<br>discriminator/d_h4_conv/w:0 (float32_ref 5x5x512x1024) [13107200, bytes: 52428800]<br>discriminator/d_h4_conv/biases:0 (float32_ref 1024) [1024, bytes: 4096]<br>discriminator/d_bn4/beta:0 (float32_ref 1024) [1024, bytes: 4096]<br>discriminator/d_bn4/gamma:0 (float32_ref 1024) [1024, bytes: 4096]<br>discriminator/d_h5_lin/Matrix:0 (float32_ref 16384x1) [16384, bytes: 65536]<br>discriminator/d_h5_lin/bias:0 (float32_ref 1) [1, bytes: 4]<br>Total size of variables: 36507524<br>Total bytes of variables: 146030096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the loss functions for the 2 models as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "d_loss_real = tf.reduce_mean(sigmoid_cross_entropy_with_logits(D_logits, tf.ones_like(D)))\n",
    "d_loss_fake = tf.reduce_mean(sigmoid_cross_entropy_with_logits(D_logits_, tf.zeros_like(D_)))\n",
    "g_loss = tf.reduce_mean(sigmoid_cross_entropy_with_logits(D_logits_, tf.ones_like(D_)))```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where, <br>\n",
    "d_loss_real is the loss for the real images passing through the discriminator<br>\n",
    "d_loss_fake is the loss for the fake images passing through the discriminator<br>\n",
    "g_loss is the loss for the images generated by the generator<br>\n",
    "\n",
    "The total loss of the discriminator (d_loss) is the sum of d_loss_real and d_loss_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Adam Optimizer to optimize the generator and discriminator models. They are defined as follows:<br>\n",
    "```python\n",
    "d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1).minimize(self.d_loss, var_list=self.d_vars)\n",
    "g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1).minimize(self.g_loss, var_list=self.g_vars)\n",
    "```\n",
    "<br>\n",
    "Learning Rate is 0.0001 and decay(beta1) is 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tActivation Function: tanH for Generator and Sigmoid for Discriminator\n",
    "2.\tCost Function: Sigmoid with Cross Entorpy\n",
    "3.\tGradient Descent: Adam Optimizer with learning rate: 0.0001 & beta1(decay rate of 1st moment estimation): 0.5\n",
    "4.\tNetwork Architecture: 5-layer Neural Network\n",
    "5.\tNetwork Initializer: random normal initializer\n",
    "6.\tBatch Size: 25\n",
    "7.\tTotal Images: 100\n",
    "8.\tEpochs: 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Code\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the code, on your terminal navigate to the installed path and run\n",
    "``` python main.py --train --crop ```\n",
    "\n",
    "This will automatically pick-up the training images present in the folder ```./Data/Simpsons_64```.\n",
    "In order to use a different data set, place the images in the folder ```./Data``` folder and change the name of the \"dataset\" flag in ```main.py``` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch: [ 0] [   0/   4] time: 9.3660, d_loss: 0.00804730, g_loss: 7.05201912<br>\n",
    "Epoch: [ 0] [   1/   4] time: 10.9825, d_loss: 0.09658723, g_loss: 8.51840591<br>\n",
    "Epoch: [ 0] [   2/   4] time: 12.5638, d_loss: 0.01051401, g_loss: 6.69371843<br>\n",
    "Epoch: [ 0] [   3/   4] time: 14.2115, d_loss: 3.46202755, g_loss: 3.73002696<br>\n",
    "Epoch: [ 1] [   0/   4] time: 15.7351, d_loss: 0.07472128, g_loss: 4.96042156<br>\n",
    "Epoch: [ 1] [   1/   4] time: 17.2322, d_loss: 0.00944371, g_loss: 5.74052477<br>\n",
    "Epoch: [ 1] [   2/   4] time: 18.7593, d_loss: 0.00912661, g_loss: 5.82239866<br>\n",
    "Epoch: [ 1] [   3/   4] time: 20.2981, d_loss: 1.34175253, g_loss: 5.06495857<br>\n",
    "Epoch: [ 2] [   0/   4] time: 21.8085, d_loss: 0.01723327, g_loss: 4.79014826<br>\n",
    "Epoch: [ 2] [   1/   4] time: 23.3216, d_loss: 0.01660232, g_loss: 4.81151104<br>\n",
    "Epoch: [ 2] [   2/   4] time: 24.8221, d_loss: 0.00530944, g_loss: 6.86225700<br>\n",
    "Epoch: [ 2] [   3/   4] time: 26.3493, d_loss: 0.04552327, g_loss: 4.08214474<br>\n",
    "Epoch: [ 3] [   0/   4] time: 27.8628, d_loss: 0.06854818, g_loss: 3.71761250<br>\n",
    "Epoch: [ 3] [   1/   4] time: 29.3643, d_loss: 0.02385384, g_loss: 4.90426350<br>\n",
    "Epoch: [ 3] [   2/   4] time: 30.8568, d_loss: 0.03424166, g_loss: 4.69303417<br>\n",
    "Epoch: [ 3] [   3/   4] time: 32.3743, d_loss: 0.01002755, g_loss: 5.53210688<br>\n",
    "Epoch: [ 4] [   0/   4] time: 33.8728, d_loss: 0.01985748, g_loss: 5.55863380<br>\n",
    "Epoch: [ 4] [   1/   4] time: 35.3774, d_loss: 0.01248339, g_loss: 5.53703785<br>\n",
    "Epoch: [ 4] [   2/   4] time: 36.9025, d_loss: 0.02969375, g_loss: 4.45696592<br>\n",
    "Epoch: [ 4] [   3/   4] time: 38.4351, d_loss: 0.21307696, g_loss: 2.91995859<br>\n",
    "Epoch: [ 5] [   0/   4] time: 39.9196, d_loss: 0.16518828, g_loss: 2.64963651<br>\n",
    "Epoch: [ 5] [   1/   4] time: 41.4348, d_loss: 0.01804122, g_loss: 6.06599474<br>\n",
    "Epoch: [ 5] [   2/   4] time: 42.9457, d_loss: 0.01435683, g_loss: 6.68838930<br>\n",
    "Epoch: [ 5] [   3/   4] time: 44.4653, d_loss: 0.00883127, g_loss: 7.26980829<br>\n",
    "Epoch: [ 6] [   0/   4] time: 45.9689, d_loss: 0.01340097, g_loss: 5.95355368<br>\n",
    "Epoch: [ 6] [   1/   4] time: 47.4735, d_loss: 0.04442319, g_loss: 3.88450599<br>\n",
    "Epoch: [ 6] [   2/   4] time: 48.9736, d_loss: 0.04200076, g_loss: 3.81001735<br>\n",
    "Epoch: [ 6] [   3/   4] time: 50.4751, d_loss: 0.02555417, g_loss: 4.34248734<br>\n",
    "Epoch: [ 7] [   0/   4] time: 51.9833, d_loss: 0.09478149, g_loss: 3.46468067<br>\n",
    "Epoch: [ 7] [   1/   4] time: 53.4773, d_loss: 0.03278716, g_loss: 4.23916864<br>\n",
    "Epoch: [ 7] [   2/   4] time: 54.9713, d_loss: 0.04215960, g_loss: 4.22080708<br>\n",
    "Epoch: [ 7] [   3/   4] time: 56.4849, d_loss: 0.02984809, g_loss: 4.95883036<br>\n",
    "Epoch: [ 8] [   0/   4] time: 57.9754, d_loss: 0.02219681, g_loss: 5.00462198<br>\n",
    "Epoch: [ 8] [   1/   4] time: 59.4845, d_loss: 0.02506564, g_loss: 4.62666941<br>\n",
    "Epoch: [ 8] [   2/   4] time: 60.9863, d_loss: 0.05980067, g_loss: 4.34014511<br>\n",
    "Epoch: [ 8] [   3/   4] time: 62.5002, d_loss: 0.02720683, g_loss: 5.18341923<br>\n",
    "Epoch: [ 9] [   0/   4] time: 64.0038, d_loss: 0.01533393, g_loss: 5.67472982<br>\n",
    "Epoch: [ 9] [   1/   4] time: 65.5188, d_loss: 0.01708235, g_loss: 5.04262686<br>\n",
    "Epoch: [ 9] [   2/   4] time: 67.0179, d_loss: 0.01435666, g_loss: 5.96386719<br>\n",
    "Epoch: [ 9] [   3/   4] time: 68.5394, d_loss: 0.02204786, g_loss: 4.83817005<br>\n",
    "Epoch: [10] [   0/   4] time: 70.0413, d_loss: 0.01232358, g_loss: 5.61192083<br>\n",
    "Epoch: [10] [   1/   4] time: 71.5432, d_loss: 0.00669380, g_loss: 6.16942406<br>\n",
    "Epoch: [10] [   2/   4] time: 73.0393, d_loss: 0.00966259, g_loss: 6.14368629<br>\n",
    "Epoch: [10] [   3/   4] time: 74.5814, d_loss: 0.03318290, g_loss: 4.32485056<br>\n",
    "Epoch: [11] [   0/   4] time: 76.0770, d_loss: 0.01441771, g_loss: 5.40539503<br>\n",
    "Epoch: [11] [   1/   4] time: 77.5875, d_loss: 0.00369535, g_loss: 7.03609514<br>\n",
    "Epoch: [11] [   2/   4] time: 79.1080, d_loss: 0.00759749, g_loss: 6.58801556<br>\n",
    "Epoch: [11] [   3/   4] time: 80.6081, d_loss: 0.01224802, g_loss: 5.92280626<br>\n",
    "Epoch: [12] [   0/   4] time: 82.1091, d_loss: 0.01170493, g_loss: 5.17165852<br>\n",
    "Epoch: [12] [   1/   4] time: 83.6086, d_loss: 0.01223969, g_loss: 5.80764532<br>\n",
    "Epoch: [12] [   2/   4] time: 85.1212, d_loss: 0.00818410, g_loss: 6.62704229<br>\n",
    "Epoch: [12] [   3/   4] time: 86.6609, d_loss: 0.02226664, g_loss: 5.19087696<br>\n",
    "Epoch: [13] [   0/   4] time: 88.1745, d_loss: 0.01443872, g_loss: 5.19868040<br>\n",
    "Epoch: [13] [   1/   4] time: 89.6758, d_loss: 0.00672846, g_loss: 5.74825907<br>\n",
    "Epoch: [13] [   2/   4] time: 91.1935, d_loss: 0.01081088, g_loss: 5.57111931<br>\n",
    "Epoch: [13] [   3/   4] time: 92.7240, d_loss: 0.03427375, g_loss: 4.19021749<br>\n",
    "Epoch: [14] [   0/   4] time: 94.2232, d_loss: 0.02590284, g_loss: 4.46787453<br>\n",
    "Epoch: [14] [   1/   4] time: 95.7166, d_loss: 0.01754840, g_loss: 4.91180038<br>\n",
    "Epoch: [14] [   2/   4] time: 97.2247, d_loss: 0.01751318, g_loss: 5.16031599<br>\n",
    "Epoch: [14] [   3/   4] time: 98.7553, d_loss: 0.01427595, g_loss: 5.58691168<br>\n",
    "Epoch: [15] [   0/   4] time: 100.2583, d_loss: 0.01843244, g_loss: 4.40491199<br>\n",
    "Epoch: [15] [   1/   4] time: 101.7646, d_loss: 0.01243536, g_loss: 4.77438831<br>\n",
    "Epoch: [15] [   2/   4] time: 103.2936, d_loss: 0.10667857, g_loss: 3.15785646<br>\n",
    "Epoch: [15] [   3/   4] time: 104.8106, d_loss: 0.02200819, g_loss: 5.98732615<br>\n",
    "Epoch: [16] [   0/   4] time: 106.3201, d_loss: 0.00618810, g_loss: 6.95014572<br>\n",
    "Epoch: [16] [   1/   4] time: 107.8276, d_loss: 0.00112494, g_loss: 8.30367088<br>\n",
    "Epoch: [16] [   2/   4] time: 109.3518, d_loss: 0.00449583, g_loss: 8.07813263<br>\n",
    "Epoch: [16] [   3/   4] time: 110.8543, d_loss: 0.03454046, g_loss: 4.24285698<br>\n",
    "Epoch: [17] [   0/   4] time: 112.3485, d_loss: 0.01515299, g_loss: 5.38927269<br>\n",
    "Epoch: [17] [   1/   4] time: 113.8427, d_loss: 0.00462707, g_loss: 7.06467533<br>\n",
    "Epoch: [17] [   2/   4] time: 115.3346, d_loss: 0.00884411, g_loss: 7.17192888<br>\n",
    "Epoch: [17] [   3/   4] time: 116.8628, d_loss: 0.01973029, g_loss: 6.27611876<br>\n",
    "Epoch: [18] [   0/   4] time: 118.3682, d_loss: 0.00860556, g_loss: 6.71839857<br>\n",
    "Epoch: [18] [   1/   4] time: 119.9160, d_loss: 0.00869883, g_loss: 6.15110826<br>\n",
    "Epoch: [18] [   2/   4] time: 121.4120, d_loss: 0.00928370, g_loss: 6.09283829<br>\n",
    "Epoch: [18] [   3/   4] time: 122.9368, d_loss: 0.00822398, g_loss: 6.66194439<br>\n",
    "Epoch: [19] [   0/   4] time: 124.4292, d_loss: 0.00291626, g_loss: 7.10071659<br>\n",
    "Epoch: [19] [   1/   4] time: 125.9427, d_loss: 0.00230291, g_loss: 6.81967115<br>\n",
    "Epoch: [19] [   2/   4] time: 127.4466, d_loss: 0.02431140, g_loss: 4.78234243<br>\n",
    "Epoch: [19] [   3/   4] time: 128.9633, d_loss: 0.03422339, g_loss: 4.20858574<br>\n",
    "Epoch: [20] [   0/   4] time: 130.4566, d_loss: 0.00712587, g_loss: 6.09566545<br>\n",
    "Epoch: [20] [   1/   4] time: 131.9585, d_loss: 0.00334194, g_loss: 7.37051392<br>\n",
    "Epoch: [20] [   2/   4] time: 133.4581, d_loss: 0.00596511, g_loss: 7.39865494<br>\n",
    "Epoch: [20] [   3/   4] time: 134.9832, d_loss: 2.47153139, g_loss: 2.25520945<br>\n",
    "Epoch: [21] [   0/   4] time: 136.5024, d_loss: 0.11012243, g_loss: 14.14280987<br>\n",
    "Epoch: [21] [   1/   4] time: 137.9874, d_loss: 0.22035009, g_loss: 17.17830276<br>\n",
    "Epoch: [21] [   2/   4] time: 139.4850, d_loss: 0.05087453, g_loss: 10.32483673<br>\n",
    "Epoch: [21] [   3/   4] time: 141.0066, d_loss: 0.56518769, g_loss: 2.98864126<br>\n",
    "Epoch: [22] [   0/   4] time: 142.5077, d_loss: 0.02509777, g_loss: 11.04115486<br>\n",
    "Epoch: [22] [   1/   4] time: 144.0092, d_loss: 0.02014254, g_loss: 11.68441391<br>\n",
    "Epoch: [22] [   2/   4] time: 145.5148, d_loss: 0.85586715, g_loss: 8.93093586<br>\n",
    "Epoch: [22] [   3/   4] time: 147.0386, d_loss: 0.69659758, g_loss: 1.79383457<br>\n",
    "Epoch: [23] [   0/   4] time: 148.5474, d_loss: 0.05355394, g_loss: 4.31195784<br>\n",
    "Epoch: [23] [   1/   4] time: 150.0470, d_loss: 0.05076646, g_loss: 4.59662247<br>\n",
    "Epoch: [23] [   2/   4] time: 151.5471, d_loss: 0.08791910, g_loss: 3.80403996<br>\n",
    "Epoch: [23] [   3/   4] time: 153.0592, d_loss: 0.08182263, g_loss: 2.92726135<br>\n",
    "Epoch: [24] [   0/   4] time: 154.5483, d_loss: 0.10160693, g_loss: 2.63836312<br>\n",
    "Epoch: [24] [   1/   4] time: 156.1186, d_loss: 0.04913354, g_loss: 3.74601221<br>\n",
    "Epoch: [24] [   2/   4] time: 157.6297, d_loss: 0.02884583, g_loss: 4.49800587<br>\n",
    "[Sample] d_loss: 0.00623855, g_loss: 5.96150637"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gist of the model training code is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    # Update D network\n",
    "     _, summary_str = self.sess.run([d_optim, self.d_sum],\n",
    "    feed_dict={ self.inputs: batch_images, self.z: batch_z })\n",
    "    # Update G network\n",
    "    _, summary_str = self.sess.run([g_optim, self.g_sum],\n",
    "    feed_dict={ self.z: batch_z })\n",
    "\n",
    "    # Run g_optim twice to make sure that d_loss does not go to zero (different from paper)\n",
    "    _, summary_str = self.sess.run([g_optim, self.g_sum],\n",
    "    feed_dict={ self.z: batch_z })\n",
    "          \n",
    "    errD_fake = self.d_loss_fake.eval({ self.z: batch_z })\n",
    "    errD_real = self.d_loss_real.eval({ self.inputs: batch_images })\n",
    "    errG = self.g_loss.eval({self.z: batch_z})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At every step, we try to optimize the sum of the network, where the sum is the sum of the losses.<br>\n",
    "In the discriminator's case, it is the loss of the real images, and in generator's case, it is the loss of the fake images.<br>\n",
    "\n",
    "It is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "self.g_sum = tf.summary.merge([self.z_sum, self.d__sum,\n",
    "      self.G_sum, self.d_loss_fake_sum, self.g_loss_sum])\n",
    "self.d_sum = tf.summary.merge([self.z_sum, self.d_sum, \n",
    "      self.d_loss_real_sum, self.d_loss_sum])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The losses are captured in Tensorboard:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Discriminator loss](./images/d_loss.PNG)\n",
    "**Fig. 1 : Discriminator Loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Generator loss](./images/g_loss.PNG)\n",
    "**Fig. 2 : Generator Loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the loss of the 2 are kind of inversely related (Like adversaries). <br>\n",
    "If the discriminator has a lower loss, it means it can distinguish the fake images from the real ones, which in turn means the generator cannot produce good quality output, and vice-versa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Output\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GAN](./images/GAN.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Output\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Test Output](./images/test_20180425010142.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the output generated by the test, it is evident that the model had started to distinguish between various Simpson’s characters and tried to generate a new face based off the existing ones.\n",
    "The model did lose its track around 600 epoch, where in started generating noise instead of faces. It stabilized in some 400 epochs, and eventually started producing better outputs again around the 1000 epoch.\n",
    "\n",
    "With enough images and more training, I think the model would be stable enough to generate better output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Scope\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GAN model while having many applications still isn’t stable enough to generate definitive results.<br>\n",
    "Model is susceptible to mode collapse, where in once the generator can fool the discriminator, it keeps on producing similar results again and again. <br><br>\n",
    "GAN models also suffer from convergence, and therefore we don’t know when to stop training. To overcome this, there was a paper proposing use of Wasserstein distance instead of Jensen-Shannon divergence to understand the loss function better, which can be correlated to image quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Test Output](./images/WGAN.png)\n",
    "**Fig. 3: Loss functions in WGAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another new search in the field of neural networks gave rise to Capsule Networks, which are evidently much better than CNNs in training models.<br>\n",
    "These networks can be used in place of CNNs in the GAN architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tGenerative Adversarial Networks (https://arxiv.org/abs/1406.2661)<br>\n",
    "2.\tGAN tutorial: https://medium.com/@awjuliani/generative-adversarial-networks-explained-with-a-classic-spongebob-squarepants-episode-54deab2fce39.<br>\n",
    "3.\tGenerative models: https://en.wikipedia.org/wiki/Generative_model<br>\n",
    "4.\tDiscriminative models : https://en.wikipedia.org/wiki/Discriminative_model<br>\n",
    "5.\tCNN: http://cs231n.github.io/convolutional-networks/<br>\n",
    "6.\tDCGAN: https://github.com/carpedm20/DCGAN-tensorflow<br>\n",
    "7.\tHacks for GAN: https://github.com/soumith/ganhacks<br>\n",
    "8.\thttps://arxiv.org/abs/1511.06434<br>\n",
    "9.\thttps://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6<br>\n",
    "10. https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/<br>\n",
    "11. http://gluon.mxnet.io/chapter14_generative-adversarial-networks/dcgan.html<br>\n",
    "12. WGAN https://arxiv.org/abs/1701.07875<br>\n",
    "13. https://hackernoon.com/what-is-a-capsnet-or-capsule-network-2bfbe48769cc<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Licenses\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text in the document by Neel Indap is licensed under CC BY 3.0 https://creativecommons.org/licenses/by/3.0/us/\n",
    "\n",
    "The code in the document by Neel Indap is licensed under the MIT License https://opensource.org/licenses/MIT\n",
    "\n",
    "![License](https://licensebuttons.net/l/by/3.0/us/88x31.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
